{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a intuitive explaination of the mathematics behind Representation Learning with Contrastive Predictive Coding (CPC) https://arxiv.org/pdf/1807.03748.pdf by using CPC to learn representations of sound files for the purpose of speech recognition.\n",
    "\n",
    "The title Contrastive Predictive Coding is itself a good summary of the 3 importance concepts of the paper.\n",
    "\n",
    "*Contrastive*: The Objective Function incorporates a version of Noise Contrastive Estimation Whereby the acheive a better, lower loss value, the model must generate a vector that is both similar to the target vector and simultaneously dissimilar to some number of negative samples. In the example here, the batch itself is used as the positive and negative samples. \n",
    "\n",
    "*Predictive*: The model learns high level representations of each timestep and of sequences by predicting several timesteps into the future \n",
    "\n",
    "*Coding*: The model learns to predict the future not at the level of the low level high dimensional inputs but at the level of high level latent space that the inputs are encoded into.  \n",
    "\n",
    "The CPC paper combines these concepts to learn meaningful representations of sequences, images and reinforcement learning states. \n",
    "\n",
    "When I read this paper and tried to explain it, I felt that one particular idea was the bottelneck in understanding the paper well enough to connect the theory with the code. That idea was the relationship between the density ratio and the linear transformation of z_t+k into a scalar and taking the exponent of that scalar.\n",
    "\n",
    "This is what i mean, the paper states that instead of modeling a generative model such as p(x|c), it models a density ratio as a log-bilinear model.\n",
    "\n",
    "$$\\frac{P(x_{t+k}|c_t)}{P(x_{t+k})} \\approx e^{z^T_{t+k} W_k c_t)} $$\n",
    "\n",
    "Where z is an encoding of x, much like texts are an encoding of speech. Here z_t+k means the z's in the future relative to a timepoint t. c_t is a summary of series of z's from z_0 to z_t, and W is a matrix transformation to make z and c the same size so that they can be compared. \n",
    "\n",
    "Here is an visual explaination of the density ratio:\n",
    "\n",
    "consider this image, lets call this image X:\n",
    "\n",
    "<img src=\"https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2019/04/Example-of-a-GAN-Generated-MNIST-Handwritten-Digit-for-a-Vector-of-Zeros-300x225.png\">\n",
    "\n",
    "It looks like either an 8 or a 9. Its kind strange looking, and you probably have seen something like it once or twice in your early years in kindergarten but in your lifespan you typically see better written numbers. If you asked a person to draw a number, any number, what is the probability they draw this image? \n",
    "\n",
    "The probability of X, P(X) is therefore lower than a clearly written 8 or 9. If P(8) or P(9) is around 0.1, then P(X) is more like 0.05\n",
    "\n",
    "The variable \"c\" is an idea of the number that the pixels in X represent, it is a high level concept, it doesnt bother itself with arranging the pixels in X just right and low level tasks of getting all the correlations between pixels just right to draw out a number. Imagine c = \"9\", c = \"8\" and c = \"7\". Clearly X is more likely 8 or 9 than it is 7. so P(X| c = 8) > P (X| c = 7).\n",
    "\n",
    "So is c = 8 a good \"idea\" for what X is ? Since X is kinda strange P(X| c = 8) is still pretty low, say P(X| c = 8) = 0.01 , thats pretty terrible, and not really fair to c = 8, since the idea of \"8\" is not only an actualy number, as opposed to \"A\", it is also what a kindergartener might draw when you ask them to draw 8. \n",
    "\n",
    "So lets normalize this by dividing by P(x), this ratio gives us a better idea of how good c is at representing the higher level concept of X. P(X| c = 8)/P(x) = 0.01/0.05 = 0.2, thats fairer.\n",
    "\n",
    "This idea of \"How good c is at representing the higher level concept of X\" is what we need to make into a tangible expression using the vectors outputs of our neural network.  \n",
    "\n",
    "One way to tell if two vectors are similar, is to take their dot product. You can also take their cosine similarity. \n",
    "\n",
    "A neural network does several linear transformations with nonlinear transformations in between to overall, after training, do some complex transformations. If c_t holds information about z_t+k good enough to predict it, then you should be able to transform c_t into a vector that has a higher dot product with z_t+k than some random z_neg that is not in the future relative to c_t. Thats where this expression comes into play:\n",
    "\n",
    "$$ e^{z^T_{t+k} W_k c_t} $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you are using PyTorch version  1.4.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "import logging\n",
    "from timeit import default_timer as timer\n",
    "import soundfile as sf  \n",
    "\n",
    "## Libraries\n",
    "import numpy as np\n",
    "\n",
    "## Torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils import data as datautil\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "print('you are using PyTorch version ',torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "From the website http://www.openslr.org/12/ , the files dev-clean.tar.gz and train-clean-100.tar.gz are downloaded, unpacked and placed into validation and training folders respectively. Each one unpacks into a folder called LibriSpeech but the contents are not the same. Inside this folder there will be a folder called \"dev-clean\" and \"train-clean-100\" for the validation and training set respectively. The paths to these folder are what you need to provide to `RawDataset` as in the demonstration below, in order to train our CPC model. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16000 [0.00436401 0.0032959  0.00314331 ... 0.00466919 0.00604248 0.00598145] (31440,)\n",
      "use_cuda is True\n"
     ]
    }
   ],
   "source": [
    "# Touch the data\n",
    "# 19 is the speaker ID, 198 is the chapter ID, CHAPTERS.TXT has the name of the book chapter\n",
    "# LibriSpeech/train-clean-100/19/198/19-198.trans.txt has a line by line text translation of\n",
    "# all of the flac audio files\n",
    "path = 'data/training/LibriSpeech/train-clean-100/19/198/19-198-0000.flac'    \n",
    "audiodata, samplerate = sf.read(path)  # samplerate = 16kHz \n",
    "print(samplerate, audiodata, audiodata.shape)\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print('use_cuda is', use_cuda)\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RawDataset(datautil.Dataset):\n",
    "    \n",
    "    def __init__(self, directory, audio_window):\n",
    "\n",
    "        self.audio_window = audio_window \n",
    "        self.audiolist = []\n",
    "        self.idx2file = {}\n",
    "        self.files = []\n",
    "        # r=root, d=directories, f = files\n",
    "        for r, d, f in os.walk(directory):\n",
    "            for file in f:\n",
    "                if '.flac' in file:\n",
    "                    self.files.append(os.path.join(r, file))\n",
    "\n",
    "        for idx, filepath in enumerate(self.files):\n",
    "            self.idx2file[idx] = filepath\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Denotes the total number of utterances \"\"\"\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        filepath = self.idx2file[index]\n",
    "        audiodata, samplerate = sf.read(filepath)\n",
    "        utt_len = audiodata.shape[0] \n",
    "        # get the index to read part of the utterance into memory \n",
    "        index = np.random.randint(utt_len - self.audio_window + 1) \n",
    "        return audiodata[index:index+self.audio_window]\n",
    "    \n",
    "def save_model(model,name):\n",
    "    torch.save(model.state_dict(),name)\n",
    "    \n",
    "def load_model(model,name):\n",
    "    model.load_state_dict(torch.load(name))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = RawDataset(directory = \"data/training/LibriSpeech/train-clean-100\", \n",
    "                          audio_window = 20480)\n",
    "\n",
    "validation_set = RawDataset(directory = \"data/validation/LibriSpeech/dev-clean\", \n",
    "                          audio_window = 20480)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScheduledOptim(object):\n",
    "    \"\"\"A simple wrapper class for learning rate scheduling\"\"\"\n",
    "\n",
    "    def __init__(self, optimizer, n_warmup_steps):\n",
    "        self.optimizer = optimizer\n",
    "        self.d_model = 128 \n",
    "        self.n_warmup_steps = n_warmup_steps\n",
    "        self.n_current_steps = 0 \n",
    "        self.delta = 1\n",
    "\n",
    "    def state_dict(self):\n",
    "        self.optimizer.state_dict()\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"Step by the inner optimizer\"\"\"\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def zero_grad(self):\n",
    "        \"\"\"Zero out the gradients by the inner optimizer\"\"\"\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "    def increase_delta(self):\n",
    "        self.delta *= 2\n",
    "\n",
    "    def update_learning_rate(self):\n",
    "        \"\"\"Learning rate scheduling per step\"\"\"\n",
    "\n",
    "        self.n_current_steps += self.delta\n",
    "        new_lr = np.power(self.d_model, -0.5) * np.min([\n",
    "            np.power(self.n_current_steps, -0.5),\n",
    "            np.power(self.n_warmup_steps, -1.5) * self.n_current_steps])\n",
    "\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['lr'] = new_lr\n",
    "        return new_lr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Encoder-AutoRegressive Model\n",
    "\n",
    "The raw waveform is fed to the encoder without preprocessing. The size 20480 audio segment represents a 1.28 second long speech, or audio signal. The 160 downsampling factor for the encoder takes a 20480 sequence and compresses it to sequence length 128. \n",
    "\n",
    "When batch_first = True, the GRU input and output shapes are (batch, seq, feature), and this is the shape of z which the GRU takes as input, only that features is equivalent to the channel dimension in 1D CNNs, so z is shpae (batch_size,seq_len,channels). \n",
    "\n",
    "The Noise Contrastive Estimation is acheived by using the other samples in the batch as the negative samples. \n",
    "\n",
    "What we learn from this implementation is that the expression \n",
    "\n",
    "$$W_k c_t$$\n",
    "\n",
    "in the log-bilinear model, is meant to be an approximation of \n",
    "\n",
    "$$z_{t+k}$$\n",
    "\n",
    "And so the higher the dot product is between the two, the higher is the density ratio estimate f_k, where\n",
    "\n",
    "$$f_k(x_{t+k}, c_t) = exp(z^T_{t+k} W_k c_t)$$\n",
    "\n",
    "If you look at the implementation below, you will not see the exponent term, because this exponent term is incorporated into the log softmax term. \n",
    "\n",
    "In each training batch, batch_size samples of audio signal is used of size 20480x1. The encoder turns this sequence into shape 128x512 (z). each size 512 vector in z is 0.01 seconds, which is why 128 of them amounts to 1.28 seconds. \n",
    "\n",
    "`time_C` is a number chosen between 0 and 128 - K, so if K = 12, then `time_C` is between 0 amd 116. Suppose `time_C` = 100, then the vector c_t of size 256 is an embedding of the first 100 vectors of z or the first 1.0 seconds of the audio signal. \n",
    "\n",
    "c_t is used to predict the next K of z_t's in the future. ie z_t+k. If c_t actually holds enough information to predict z_t+k, then some linear transformation W_k should be able to approximate z_t+k from c_t. \n",
    "\n",
    "The entire 128 z's are encoded into c's by the same encoder, then a random timstep is chosen from the first 128 - K timsteps in order to predict the next K timesteps, which is why the random timestep `time_C` cannot be higher than 128 - K, or else there will not be a total of K timesteps in the future to compare to. \n",
    "\n",
    "# The InfoNCE loss function\n",
    "\n",
    "If the encoder and the GRU have generated a good representation c_t, then the dot product of W_k c_t with z_t+k should be higher than tha dot product of W_k c_t with zneg_t+k, where zneg_t+k is the z vector by from a different sample in the batch. This is the \"negative sample\" in \"negative sampling\". \n",
    "\n",
    "In the implementation below the line `zWc = torch.mm(z_t_k[k], torch.transpose(W_c_k[k],0,1))` is performed for each future timestep k in K. \n",
    "\n",
    "The z_t_k tensor is the K number of z_t+k's for each sample in the batch.\n",
    "W_c_k is the K number of vectors that are a result of matrix multiplying Wk with ct that is supposed to approximate z_t_k. The transpose operation is performed on W_c_k in order for the zWc calculation to be a calculation of shape (batch_size, z_size)x(z_size, batch_size) => (batch_size, batch_size). So W_c_k is not just dot producted with the z_t_k it was meant to approximate, it is also dot producted with the z_t_k's fromt he other samples in the batch. The value in position (k,k) of zWc is proportional to how well W_c_k approximates z_t_k, the values in position (k, not k) is proportional to how well W_c_k approximates not the z_t_k from it's own sequence, but the z_t_k from some other sequece, which it should not be good at it it is working properly. \n",
    "\n",
    "Suppose batch size is 3 and the first row of zWc are the elements `[0.1,  1,  -0.1]` then the log_softmax is `[-1.4536, -0.5536, -1.6536]`. This means that W_c_k has a weak preference for it's own z_t_k, over the last, 3rd  z_t_k, which is good, but it has a much stronger preference for the 2nd z_t_k than it's own, which is bad. The softmax function forces the entire row to be less and 1 and as a whole sum to 1. If you want the first element in the softmax to be close to 1.0, which you want, since this means that the negative loss will be close to 0, (the negative loss in this case will be +1.4536) then you want not only for the first element to be large, that is not enough, the other elements need to be low. This is why the loss function log_softmax(z_t_k dot W_c_k / sum of z_t_k dot W_c_k for all samples) not only helps to tune your neural network to move in the right direction, but move away from the wrong directions.\n",
    "\n",
    "Here I show you which line in the code corresponds to the loss function described int he paper:\n",
    "\n",
    "$$ InfoNCELoss(z_{t = t' \\rightarrow t'+k}, c_t) = - E \\left[  \\log( \\frac{f_k(z_{t+k}, c_t)}{\\sum_{J} f_k(z_{j}, c_t)} ) \\right] $$\n",
    "\n",
    "$$ = - E \\left[  \\log( \\frac{e^{z^T_{t+k} W_k c_t}}{\\sum_{J} e^{z^T_{j} W_k c_t}} ) \\right]$$\n",
    "\n",
    "$$ = - \\frac{1}{KJ} \\sum_{J}\\sum_{K} \\log( \\frac{e^{z^T_{t+k} W_k c_t}}{\\sum_{J} e^{z^T_{j} W_k c_t}} ) $$\n",
    "\n",
    "$$ = - E \\left[  \\log( softmax( e^{z^T_{j} W_k c_t)}_i ) \\right]  $$\n",
    "\n",
    "Where i is the index of the correct element in the softmax, the code version of this is:\n",
    "\n",
    "`nce += torch.sum(torch.diag(logsof_zWc))`\n",
    "\n",
    "`nce /= -1.*batch_size*self.K` \n",
    "\n",
    "The fact that the nce is accumulated over each sample of the batch and each future timestep k, then at the end divided by `batch_size*self.K` represents the expectation. \n",
    "\n",
    "The `torch.diag(logsof_zWc)` makes a vector from the diagonal of the `logsof_zWc` matrix, so it is taking only the log softmax values of the correct z_t_k dot W_c_k pairs (this value through the softmax has already incorporated information from the neighboring negative samples). Just like the softmax term is only considering the ratio of the element over the elements including itself:\n",
    "\n",
    "$$\\frac{f_k(z_{t+k}, c_t)}{\\sum_{J} f_k(z_{j}, c_t)}$$ \n",
    "\n",
    "Here I use z instead of x for simplicity and making the code look like the formula, the paper uses x in the density ratio formula f_k. But z is a direct mapping from x, so hopefully you can see that it is the same. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CPC(nn.Module):\n",
    "    def __init__(self, K, seq_len):\n",
    "        \"\"\"\n",
    "        K: this is K in the paper, the K timesteps in the future relative to\n",
    "           the last context timestep  are used as targets to teach the model \n",
    "           to predict K steps into the future\n",
    "        \"\"\"\n",
    "        super(CPC, self).__init__()\n",
    "\n",
    "        self.seq_len = seq_len\n",
    "        self.K = K\n",
    "        self.c_size = 256\n",
    "        self.z_size = 512\n",
    "        self.dwn_fac = 160\n",
    "        # the downsampling factor of this CNN is 160\n",
    "        # so the output sequence length is 20480/160 = 128\n",
    "        self.encoder = nn.Sequential( \n",
    "            nn.Conv1d(1, self.z_size, kernel_size=10, stride=5, padding=3, bias=False),\n",
    "            nn.BatchNorm1d(self.z_size),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(self.z_size, self.z_size, kernel_size=8, stride=4, padding=2, bias=False),\n",
    "            nn.BatchNorm1d(self.z_size),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(self.z_size, self.z_size, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm1d(self.z_size),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(self.z_size, self.z_size, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm1d(self.z_size),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(self.z_size, self.z_size, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm1d(self.z_size),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        self.gru = nn.GRU(self.z_size, self.c_size, num_layers = 1, \n",
    "                          bidirectional = False, batch_first = True)\n",
    "        \n",
    "        # These are all trained\n",
    "        self.Wk = nn.ModuleList([nn.Linear(self.c_size,self.z_size) for i in range(self.K)])\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.lsoftmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def init_hidden(self, batch_size, use_gpu=True):\n",
    "        if use_gpu: return torch.zeros(1, batch_size, self.c_size).cuda()\n",
    "        else: return torch.zeros(1, batch_size, self.c_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        \"\"\"\n",
    "        x: torch.float32 shape (batch_size,channels=1,seq_len)\n",
    "        hidden: torch.float32 \n",
    "                shape (num_layers*num_directions=1,batch_size,hidden_size=256) \n",
    "        \"\"\"\n",
    "        batch_size = x.size()[0] \n",
    "        \n",
    "        # input x is shape (batch_size, channels, seq_len), e.g. 8*1*20480\n",
    "        z = self.encoder(x) \n",
    "        # encoded sequence z is shape (batch_size, z_size, seq_len), e.g. 8*512*128\n",
    "        \n",
    "        z = z.transpose(1,2) # reshape->(batch_size,seq_len,z_size) for GRU, e.g. 8*128*512\n",
    "        \n",
    "        # pick timestep to be the last in the context, time_C, later ones are targets \n",
    "        highest = self.seq_len//self.dwn_fac - self.K # 128 -12 = 116\n",
    "        time_C = torch.randint(highest, size=(1,)).long() # some number between 0 and 116\n",
    "\n",
    "        # encode_samples (K, batch_size, z_size)ie 12,8,512, \n",
    "        z_t_k = z[:, time_C + 1:time_C + self.K + 1, :].clone().cpu().float()\n",
    "        z_t_k = z_t_k.transpose(1,0)\n",
    "        \n",
    "        z_0_T = z[:,:time_C + 1,:] # e.g. size 8*100*512\n",
    "        output, hidden = self.gru(z_0_T, hidden) # output size e.g. 8*100*256\n",
    "        c_t = output[:,time_C,:].view(batch_size, self.c_size) # c_t e.g. size 8*256\n",
    "        \n",
    "        # For the future K timesteps, predict their z_t+k, \n",
    "        W_c_k = torch.empty((self.K, batch_size, self.z_size)).float() # e.g. size 12*8*512\n",
    "        for k in np.arange(0, self.K):\n",
    "            linear = self.Wk[k] # c_t is size 256, Wk is a 512x256 matrix \n",
    "            W_c_k[k] = linear(c_t) # Wk*c_t e.g. size 8*512\n",
    "            \n",
    "        nce = 0 # average over timestep and batch\n",
    "        for k in np.arange(0, self.K):\n",
    "            \n",
    "            # (batch_size, z_size)x(z_size, batch_size) = (batch_size, batch_size)\n",
    "            zWc = torch.mm(z_t_k[k], torch.transpose(W_c_k[k],0,1)) \n",
    "            # print(zWc)\n",
    "            # total has shape (batch_size, batch_size) e.g. size 8*8\n",
    "            \n",
    "            logsof_zWc = self.lsoftmax(zWc)\n",
    "            #print(logsof_zWc)\n",
    "            nce += torch.sum(torch.diag(logsof_zWc)) # nce is a tensor\n",
    "            \n",
    "        nce /= -1.*batch_size*self.K\n",
    "        \n",
    "        argmax = torch.argmax(self.softmax(zWc), dim=0)# softmax not required if argmax \n",
    "        correct = torch.sum( torch.eq( argmax, torch.arange(0, batch_size) ) ) \n",
    "        accuracy = 1.*correct.item()/batch_size\n",
    "\n",
    "        return accuracy, nce, hidden\n",
    "\n",
    "    def predict(self, x, hidden):\n",
    "\n",
    "        # input sequence is N*C*L, e.g. 8*1*20480\n",
    "        \n",
    "        z = self.encoder(x) # encoded sequence is N*C*L, e.g. 8*512*128\n",
    "        \n",
    "        z = z.transpose(1,2) # reshape to N*L*C for GRU, e.g. 8*128*512\n",
    "        output, hidden = self.gru(z, hidden) # output size e.g. 8*128*256\n",
    "\n",
    "        return output, hidden # return every frame\n",
    "        #return output[:,-1,:], hidden # only return the last frame per utt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CPC(K = 2, seq_len = 20480).to(device)\n",
    "#load_model(model,\"weights/CPC_K2.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.007570809000753798\n"
     ]
    }
   ],
   "source": [
    "params = {}\n",
    "loader = datautil.DataLoader(training_set, batch_size=3, shuffle=True, **params)\n",
    "\n",
    "for batch_idx, utterance in enumerate(loader):\n",
    "\n",
    "    break\n",
    "    \n",
    "# needs to be float vs double, add channel dimension\n",
    "start = timer()\n",
    "utterance = utterance.float().float().unsqueeze(1).to(device) \n",
    "hidden = model.init_hidden(len(utterance), use_gpu=True)\n",
    "acc, loss, hidden = model(utterance, hidden)\n",
    "end = timer()\n",
    "print(end- start) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The softmax probability, the density ratio and mutual information\n",
    "The paper asks you to noitice that the loss function is of the same form as the cross entropy loss \n",
    "\n",
    "$$-ylog(P(y|x))$$\n",
    "\n",
    "where\n",
    "\n",
    "$$ P(y|x) \\approx \\frac{f_k(z_{t+k}, c_t)}{\\sum_{J} f_k(z_{j}, c_t)}$$  \n",
    "\n",
    "Returning to the analogy of words and emotions, the loss function is optimized when the model's internal representation of what it means to be angry maximizes P(\"hate\"|angry)/P(\"hate\").\n",
    "\n",
    "What is the probability that is the angry kind of hate and not one of the other hates like from someone scared or happy? its not P(\"hate\"|angry)/P(\"hate\") cause thats a ratio, not a probability. probabilities need to sum to 1 when you add them together with all the other probabilities, \"good\", \"hate\", \"the\".\n",
    "\n",
    "The paper presents this as \n",
    "\n",
    "$$ P(d=i|X,c_t) = \\frac{ \\frac{p(word_i|emotion_t)}{p(word_i)} }{ \\sum^{N}_{j=1} \\frac{p(word_j|emotion_t)}{p(word_j)} } $$\n",
    "\n",
    "and uses this as a justification for why minimizing the loss function in turns maximizes the density ration, which in turn maximizes the mutual information. The relationship to mutual information is proven in the Appendix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(log_interval, model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, data in enumerate(train_loader):\n",
    "        data = data.float().unsqueeze(1).to(device) # add channel dimension\n",
    "        optimizer.zero_grad()\n",
    "        hidden = model.init_hidden(len(data), use_gpu=True)\n",
    "        acc, loss, hidden = model(data, hidden)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr = optimizer.update_learning_rate()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch:{}[{}/{}({:.0f}%)]\\tlr:{:.5f}\\tAcc:{:.4f}\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), lr, acc, loss.item()))\n",
    "            \n",
    "def validation(model, device, data_loader):\n",
    "    print(\"Starting Validation\")\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_acc  = 0 \n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in data_loader:\n",
    "            data = data.float().unsqueeze(1).to(device) # add channel dimension\n",
    "            hidden = model.init_hidden(len(data), use_gpu=True)\n",
    "            acc, loss, hidden = model(data, hidden)\n",
    "            total_loss += len(data) * loss \n",
    "            total_acc  += len(data) * acc\n",
    "\n",
    "    total_loss /= len(data_loader.dataset) # average loss\n",
    "    total_acc  /= len(data_loader.dataset) # average acc\n",
    "\n",
    "    print('===> Validation set: Average loss: {:.4f}\\tAccuracy: {:.4f}\\n'.format(\n",
    "                total_loss, total_acc))\n",
    "\n",
    "    return total_acc, total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CPC(K = 2, seq_len = 20480).to(device)\n",
    "load_model(model,\"weights/CPC_K2.pth\")\n",
    "\n",
    "params = {'num_workers': 0,\n",
    "          'pin_memory': False} if use_cuda else {}\n",
    "\n",
    "train_loader = datautil.DataLoader(training_set, batch_size = 16, \n",
    "                                   shuffle=True, **params) # set shuffle to True\n",
    "\n",
    "validation_loader = datautil.DataLoader(validation_set, batch_size = 16, \n",
    "                                        shuffle=False, **params) # set shuffle to False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Validation\n",
      "===> Validation set: Average loss: 3.2256\tAccuracy: 0.2279\n",
      "\n",
      "Train Epoch:1[0/28539(0%)]\tlr:0.00025\tAcc:0.2500\tLoss: 2.091560\n",
      "Train Epoch:1[3200/28539(11%)]\tlr:0.00623\tAcc:0.5625\tLoss: 1.188438\n",
      "Train Epoch:1[6400/28539(22%)]\tlr:0.00441\tAcc:0.3750\tLoss: 1.347080\n",
      "Train Epoch:1[9600/28539(34%)]\tlr:0.00361\tAcc:0.6875\tLoss: 1.061564\n",
      "Train Epoch:1[12800/28539(45%)]\tlr:0.00312\tAcc:0.5625\tLoss: 0.853313\n",
      "Train Epoch:1[16000/28539(56%)]\tlr:0.00279\tAcc:0.8125\tLoss: 0.516153\n",
      "Train Epoch:1[19200/28539(67%)]\tlr:0.00255\tAcc:0.8750\tLoss: 0.399683\n",
      "Train Epoch:1[22400/28539(78%)]\tlr:0.00236\tAcc:0.8125\tLoss: 0.427373\n",
      "Train Epoch:1[25600/28539(90%)]\tlr:0.00221\tAcc:0.9375\tLoss: 0.664757\n",
      "Starting Validation\n",
      "===> Validation set: Average loss: 0.5609\tAccuracy: 0.8320\n",
      "\n",
      "new best val_acc 0.8320384757676655\n",
      "#### End epoch 1/1, elapsed time: 119.48127722900063\n",
      "Total elapsed time: 126.34321080900008\n"
     ]
    }
   ],
   "source": [
    "optimizer = ScheduledOptim(\n",
    "    optim.Adam(\n",
    "        filter(lambda p: p.requires_grad, model.parameters()), \n",
    "        betas=(0.9, 0.98), eps=1e-09, weight_decay=1e-4, amsgrad=True),\n",
    "        n_warmup_steps = 50)\n",
    "\n",
    "## Start training\n",
    "global_timer = timer() # global timer\n",
    "log_interval = 200\n",
    "best_acc = 0\n",
    "best_loss = np.inf\n",
    "best_epoch = -1 \n",
    "epochs = 1\n",
    "\n",
    "val_acc, val_loss = validation(model, device, validation_loader)\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    epoch_timer = timer()\n",
    "\n",
    "    # Train and validate\n",
    "    train(log_interval, model, device, train_loader, optimizer, epoch)\n",
    "    val_acc, val_loss = validation(model, device, validation_loader)\n",
    "\n",
    "    # Save\n",
    "    if val_acc > best_acc: \n",
    "        print(\"new best val_acc\", val_acc)\n",
    "        save_model(model,\"weights/CPC.pth\")\n",
    "        best_acc = max(val_acc, best_acc)\n",
    "        best_epoch = epoch + 1\n",
    "    elif epoch - best_epoch > 2:\n",
    "        optimizer.increase_delta()\n",
    "        best_epoch = epoch + 1\n",
    "\n",
    "    end_epoch_timer = timer()\n",
    "    print(\"#### End epoch {}/{}, elapsed time: {}\".format(epoch,\n",
    "                                                    epochs, \n",
    "                                                   end_epoch_timer - epoch_timer))\n",
    "\n",
    "## end \n",
    "end_global_timer = timer()\n",
    "print(\"Total elapsed time: %s\" % (end_global_timer - global_timer))\n",
    "\n",
    "save_model(model,\"weights/CPC_K2.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.4536, -0.5536, -1.6536]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsoftmax = nn.LogSoftmax(dim=1)\n",
    "lsoftmax(torch.Tensor([[0.1,  1,  -0.1]]))# [-1.4536, -0.5536, -1.6536]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
